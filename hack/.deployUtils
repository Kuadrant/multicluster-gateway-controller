# shellcheck shell=bash


# Shared functions between local-setup-mgc and quickstart-setup script

configureMetalLB () {
  clusterName=${1}
  metalLBSubnet=${2}

  kubectl config use-context kind-${clusterName}
  echo "Creating MetalLB AddressPool"
  cat <<EOF | kubectl apply -f -
apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: example
  namespace: metallb-system
spec:
  addresses:
  - 172.31.${metalLBSubnet}.0/24
---
apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: empty
  namespace: metallb-system
EOF
}

# quickstart-setup specific functions

setupAWSProvider() {
  local namespace="$1"
  if [ -z "$1" ]; then
    namespace="multi-cluster-gateways"
  fi
  if [ "$KUADRANT_AWS_ACCESS_KEY_ID" == "" ]; then
    echo "KUADRANT_AWS_ACCESS_KEY_ID is not set"
    exit 1
  fi

  kubectl apply -f - <<EOF
apiVersion: v1
kind: Secret
metadata:
  name: ${KIND_CLUSTER_PREFIX}aws-credentials
  namespace: ${namespace}
type: "kuadrant.io/aws"
stringData:
  AWS_ACCESS_KEY_ID: ${KUADRANT_AWS_ACCESS_KEY_ID}
  AWS_SECRET_ACCESS_KEY: ${KUADRANT_AWS_SECRET_ACCESS_KEY}
  AWS_REGION: ${KUADRANT_AWS_REGION}
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: ${KIND_CLUSTER_PREFIX}controller-config
  namespace: ${namespace}
data:
  AWS_DNS_PUBLIC_ZONE_ID: ${KUADRANT_AWS_DNS_PUBLIC_ZONE_ID}
  ZONE_ROOT_DOMAIN: ${KUADRANT_ZONE_ROOT_DOMAIN}
  LOG_LEVEL: "${LOG_LEVEL}"
---
apiVersion: kuadrant.io/v1alpha1
kind: ManagedZone
metadata:
  name: ${KIND_CLUSTER_PREFIX}dev-mz
  namespace: ${namespace}
spec:
  id: ${KUADRANT_AWS_DNS_PUBLIC_ZONE_ID}
  domainName: ${KUADRANT_ZONE_ROOT_DOMAIN}
  description: "Dev Managed Zone"
  dnsProviderSecretRef:
    name: ${KIND_CLUSTER_PREFIX}aws-credentials
EOF
}

setupGCPProvider() {
  local namespace="$1"
  if [ -z "$1" ]; then
    namespace="multi-cluster-gateways"
  fi
  kubectl apply -f - <<EOF
apiVersion: v1
kind: Secret
metadata:
  name: ${KIND_CLUSTER_PREFIX}gcp-credentials
  namespace: ${namespace}
type: "kuadrant.io/gcp"
stringData:
  GOOGLE: '${GOOGLE}'
  PROJECT_ID: ${PROJECT_ID}
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: ${KIND_CLUSTER_PREFIX}controller-config
  namespace: ${namespace}
data:
  ZONE_DNS_NAME: ${ZONE_DNS_NAME}
  ZONE_NAME: ${ZONE_NAME}
  LOG_LEVEL: "${LOG_LEVEL}"
---
apiVersion: kuadrant.io/v1alpha1
kind: ManagedZone
metadata:
  name: ${KIND_CLUSTER_PREFIX}dev-mz
  namespace: ${namespace}
spec:
  id: ${ZONE_NAME}
  domainName: ${ZONE_DNS_NAME}
  description: "Dev Managed Zone"
  dnsProviderSecretRef:
    name: ${KIND_CLUSTER_PREFIX}gcp-credentials
EOF
}

configureController(){
  echo "configureController ${1} ${2}"
  postDeployMGCHub ${1} ${2}
} 

postDeployMGCHub() {
    clusterName=${1}
    namespace=${2}
    kubectl config use-context kind-${clusterName}
    echo "Running post MGC deployment setup on ${clusterName}"

    case $DNS_PROVIDER in
      aws)
          echo "Setting up an AWS Route 53 DNS provider"
          setupAWSProvider ${namespace}
          ;;
      gcp)
          echo "Setting up a Google Cloud DNS provider"
          setupGCPProvider ${namespace}
          ;;
      *)
        echo "Unknown DNS provider"
        exit
        ;;
    esac
}

configureManagedAddon () {
  clusterName=${1}
  workloadCluster=${2}

  kubectl config use-context kind-${clusterName}
  echo "configureManagedAddon for workload cluster kind-${workloadCluster}"
  cat <<EOF | kubectl apply -f -
apiVersion: addon.open-cluster-management.io/v1alpha1
kind: ManagedClusterAddOn
metadata:
 name: kuadrant-addon
 namespace: kind-${workloadCluster}
spec:
 installNamespace: open-cluster-management-agent-addon
EOF
}

deployOLM(){
  clusterName=${1}

  kubectl config use-context kind-${clusterName}
  kubectl config --kubeconfig=${TMP_DIR}/kubeconfig use-context kind-${clusterName}
  echo "Installing OLM in ${clusterName}"
  ${OPERATOR_SDK_BIN} olm install --timeout 10m0s
}

deployOCMHub(){
  clusterName=${1}
  echo "installing the hub cluster in kind-(${clusterName}) "
  ${CLUSTERADM_BIN} init --bundle-version='0.11.0' --wait --context kind-${clusterName}
  echo "PATCHING CLUSTERMANAGER: placement image patch to use amd64 image - See https://kubernetes.slack.com/archives/C01GE7YSUUF/p1685016272443249"
  kubectl patch clustermanager cluster-manager --type='merge' -p '{"spec":{"placementImagePullSpec":"quay.io/open-cluster-management/placement:v0.11.0-amd64"}}' --context kind-${clusterName}
  echo "checking if cluster is single or multi"
}

deployOCMSpoke() {
  clusterName=${1}
  echo "joining the spoke cluster to the hub cluster kind-(${KIND_CLUSTER_CONTROL_PLANE}),"
  join=$(${CLUSTERADM_BIN} get token --context kind-${KIND_CLUSTER_CONTROL_PLANE} |  grep -o  'join.*--cluster-name')
  ${CLUSTERADM_BIN} ${join} kind-${clusterName} --bundle-version='0.11.0' --feature-gates=RawFeedbackJsonString=true --force-internal-endpoint-lookup --context kind-${clusterName} | grep clusteradm
  echo "accepting OCM spoke cluster invite"

  max_retry=18
  counter=0
  until ${CLUSTERADM_BIN} accept --clusters kind-${clusterName} --context kind-${KIND_CLUSTER_CONTROL_PLANE}
  do
     sleep 10
     [[ counter -eq $max_retry ]] && echo "Failed!" && exit 1
     echo "Trying again. Try #$counter"
     ((++counter))
  done
}

LOCAL_SETUP_DIR="$(dirname "${BASH_SOURCE[0]}")"

deployIngressController () {
  local INGRESS_NGINX_KUSTOMIZATION_DIR="$2"
  if [ -z "$2" ]; then
    INGRESS_NGINX_KUSTOMIZATION_DIR=${LOCAL_SETUP_DIR}/../config/ingress-nginx
  fi
  clusterName=${1}
  kubectl config use-context kind-${clusterName}
  echo "Deploying Ingress controller to ${clusterName}"
  ${KUSTOMIZE_BIN} build ${INGRESS_NGINX_KUSTOMIZATION_DIR} --enable-helm --helm-command ${HELM_BIN} | kubectl apply -f -
  echo "Waiting for deployments to be ready ..."
  kubectl -n ingress-nginx wait --timeout=600s --for=condition=Available deployments --all
}

deployThanos() {
  local THANOS_KUSTOMIZATION_DIR="$2"
  if [ -z "$2" ]; then
    THANOS_KUSTOMIZATION_DIR=${LOCAL_SETUP_DIR}/../config/thanos
  fi
  clusterName=${1}
  if [[ -n "${METRICS_FEDERATION}" ]]; then
    echo "Deploying Thanos in ${clusterName}"
    kubectl config use-context kind-${clusterName}
    ${KUSTOMIZE_BIN} build ${THANOS_KUSTOMIZATION_DIR} | kubectl apply -f -

    nodeIP=$(kubectl get nodes -o json | jq -r ".items[] | select(.metadata.name == \"$clusterName-control-plane\").status | .addresses[] | select(.type == \"InternalIP\").address")
    echo -ne "\n\n\tConnect to Thanos Query UI\n\n"
    echo -ne "\t\tURL : https://thanos-query.$nodeIP.nip.io\n\n\n"
    echo -ne "\n\n\tConnect to Grafana UI\n\n"
    echo -ne "\t\tURL : https://grafana.$nodeIP.nip.io\n\n\n"
  fi
}

deployPrometheusForFederation() {
  local PROMETHEUS_FOR_FEDERATION_KUSTOMIZATION_DIR="$2"
  if [ -z "$2" ]; then
    PROMETHEUS_FOR_FEDERATION_KUSTOMIZATION_DIR=${LOCAL_SETUP_DIR}/../config/prometheus-for-federation
  fi
  clusterName=${1}
  if [[ -n "${METRICS_FEDERATION}" ]]; then
    echo "Deploying Prometheus for federation in ${clusterName}"
    kubectl config use-context kind-${clusterName}
    # Use server-side apply to avoid below error if re-running apply
    #   'The CustomResourceDefinition "prometheuses.monitoring.coreos.com" is invalid: metadata.annotations: Too long: must have at most 262144 bytes'
    # Also need to apply the CRDs first to avoid the below error types that seem to be timing related
    #   'resource mapping not found for name: "alertmanager-main-rules" namespace: "monitoring" from "STDIN": no matches for kind "PrometheusRule" in version "monitoring.coreos.com/v1"''
    ${KUSTOMIZE_BIN} build ${PROMETHEUS_FOR_FEDERATION_KUSTOMIZATION_DIR} | ${KFILT} -i kind=CustomResourceDefinition | kubectl apply --server-side -f -
    # Apply remainder of resources
    ${KUSTOMIZE_BIN} build ${PROMETHEUS_FOR_FEDERATION_KUSTOMIZATION_DIR} | ${KFILT} -x kind=CustomResourceDefinition | kubectl apply -f -
  fi
}

deployMGCHub() {
    clusterName=${1}
    kubectl config use-context kind-${clusterName}
    echo "Initialize MGC hub setup on ${clusterName}"
    wait_for "${KUSTOMIZE_BIN} --load-restrictor LoadRestrictionsNone build ${QUICK_START_HUB_KUSTOMIZATION} --enable-helm --helm-command ${HELM_BIN} | kubectl apply -f -" "${QUICK_START_HUB_KUSTOMIZATION} control cluster config apply" "1m" "5"
    echo "Waiting for metallb-system deployments to be ready"
    kubectl -n metallb-system wait --for=condition=ready pod --selector=app=metallb --timeout=300s
    echo "Waiting for istio deployments to be ready"
    kubectl -n istio-operator wait --timeout=300s --for=condition=Available deployments --all
    kubectl -n istio-system wait --timeout=300s --for=condition=Available deployments --all
    echo "Waiting for cert-manager deployments to be ready"
    kubectl -n cert-manager wait --timeout=300s --for=condition=Available deployments --all
    echo "Waiting for kuadrant-operator deployments to be ready"
    kubectl -n kuadrant-system wait --timeout=300s --for=condition=Available deployments --all
}

deployMGCSpoke() {
   clusterName=${1}
    kubectl config use-context kind-${clusterName}
    echo "Initialize MGC spoke setup on ${clusterName}"
    wait_for "${KUSTOMIZE_BIN} --load-restrictor LoadRestrictionsNone build ${QUICK_START_SPOKE_KUSTOMIZATION} --enable-helm --helm-command ${HELM_BIN} | kubectl apply -f -" "${QUICK_START_SPOKE_KUSTOMIZATION} workload cluster config apply" "1m" "5"
    echo "Waiting for metallb-system deployments to be ready"
    kubectl -n metallb-system wait --for=condition=ready pod --selector=app=metallb --timeout=300s
    echo "Waiting for istio deployments to be ready"
    kubectl -n istio-operator wait --timeout=300s --for=condition=Available deployments --all
    kubectl -n istio-system wait --timeout=300s --for=condition=Available deployments --all
}

configureClusterAsIngress() {
    hubCluster=${1}
    spokeCluster=${2}
    # Ensure the current context points to the control plane cluster
    kubectl config use-context kind-${hubCluster}
    kubectl label managedcluster kind-${spokeCluster} ingress-cluster=true
}

deployPrometheus() {
  clusterName=${1}
  kubectl config use-context kind-${clusterName}
  echo "Deploying Prometheus in ${clusterName}"
  wait_for "${KUSTOMIZE_BIN} --load-restrictor LoadRestrictionsNone build ${PROMETHEUS_DIR} --enable-helm --helm-command ${HELM_BIN} | kubectl apply -f -" "${PROMETHEUS_DIR} cluster config apply" "1m" "5"
}
